{
 "metadata": {
  "name": "",
  "signature": "sha256:441014d93b0e3b8ffbdfb6ac982c852ff2022318ce15e8f2115f9d9ba251a60a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import cPickle\n",
      "import lasagne\n",
      "import numpy as np\n",
      "import pyprind\n",
      "import re\n",
      "import sys\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from collections import defaultdict, OrderedDict\n",
      "from theano.ifelse import ifelse\n",
      "from theano.printing import Print as pp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: GeForce GTX 745\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class GradClip(theano.compile.ViewOp):\n",
      "\n",
      "    def __init__(self, clip_lower_bound, clip_upper_bound):\n",
      "        self.clip_lower_bound = clip_lower_bound\n",
      "        self.clip_upper_bound = clip_upper_bound\n",
      "        assert(self.clip_upper_bound >= self.clip_lower_bound)\n",
      "\n",
      "    def grad(self, args, g_outs):\n",
      "        def pgrad(g_out):\n",
      "            g_out = T.clip(g_out, self.clip_lower_bound, self.clip_upper_bound)\n",
      "            g_out = ifelse(T.any(T.isnan(g_out)), T.ones_like(g_out)*0.00001, g_out)\n",
      "            return g_out\n",
      "        return [pgrad(g_out) for g_out in g_outs]\n",
      "\n",
      "gradient_clipper = GradClip(-10.0, 10.0)\n",
      "#T.opt.register_canonicalize(theano.gof.OpRemove(gradient_clipper), name='gradient_clipper')\n",
      "\n",
      "def adam(loss, all_params, learning_rate=0.001, b1=0.9, b2=0.999, e=1e-8,\n",
      "         gamma=1-1e-8):\n",
      "    \"\"\"\n",
      "    ADAM update rules\n",
      "    Default values are taken from [Kingma2014]\n",
      "\n",
      "    References:\n",
      "    [Kingma2014] Kingma, Diederik, and Jimmy Ba.\n",
      "    \"Adam: A Method for Stochastic Optimization.\"\n",
      "    arXiv preprint arXiv:1412.6980 (2014).\n",
      "    http://arxiv.org/pdf/1412.6980v4.pdf\n",
      "\n",
      "    \"\"\"\n",
      "    updates = []\n",
      "    all_grads = theano.grad(gradient_clipper(loss), all_params)\n",
      "    alpha = learning_rate\n",
      "    t = theano.shared(np.float32(1))\n",
      "    b1_t = b1*gamma**(t-1)   #(Decay the first moment running average coefficient)\n",
      " \n",
      "    for theta_previous, g in zip(all_params, all_grads):\n",
      "        m_previous = theano.shared(np.zeros(theta_previous.get_value().shape,\n",
      "                                            dtype=theano.config.floatX))\n",
      "        v_previous = theano.shared(np.zeros(theta_previous.get_value().shape,\n",
      "                                            dtype=theano.config.floatX))\n",
      " \n",
      "        m = b1_t*m_previous + (1 - b1_t)*g                             # (Update biased first moment estimate)\n",
      "        v = b2*v_previous + (1 - b2)*g**2                              # (Update biased second raw moment estimate)\n",
      "        m_hat = m / (1-b1**t)                                          # (Compute bias-corrected first moment estimate)\n",
      "        v_hat = v / (1-b2**t)                                          # (Compute bias-corrected second raw moment estimate)\n",
      "        theta = theta_previous - (alpha * m_hat) / (T.sqrt(v_hat) + e) #(Update parameters)\n",
      " \n",
      "        updates.append((m_previous, m))\n",
      "        updates.append((v_previous, v))\n",
      "        updates.append((theta_previous, theta) )\n",
      "    updates.append((t, t + 1.))\n",
      "    return updates"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class RNN(object):\n",
      "    def __init__(self,\n",
      "                 data,\n",
      "                 U,\n",
      "                 img_w=300,\n",
      "                 hidden_size=100,\n",
      "                 batch_size=50,\n",
      "                 lr_decay=0.95,\n",
      "                 sqr_norm_lim=9,\n",
      "                 non_static=True,\n",
      "                 filter_sizes=[3,4,5],\n",
      "                 num_filters=100,\n",
      "                 use_conv=False,\n",
      "                 use_lstm=True,\n",
      "                 is_bidirectional=False):\n",
      "        self.data = data\n",
      "        self.batch_size = batch_size\n",
      "        \n",
      "        img_h = data['train']['r'].shape[1]\n",
      "        \n",
      "        index = T.iscalar()\n",
      "        c = T.imatrix('c')\n",
      "        r = T.imatrix('r')\n",
      "        y = T.ivector('y')\n",
      "        c_mask = T.bmatrix('c_mask')\n",
      "        r_mask = T.bmatrix('r_mask')\n",
      "        c_seqlen = T.ivector('c_seqlen')\n",
      "        r_seqlen = T.ivector('r_seqlen')\n",
      "        embeddings = theano.shared(U, name='embeddings', borrow=True)\n",
      "        zero_vec_tensor = T.fvector()\n",
      "        self.zero_vec = np.zeros(img_w, dtype=theano.config.floatX)\n",
      "        self.set_zero = theano.function([zero_vec_tensor], updates=[(embeddings, T.set_subtensor(embeddings[0,:], zero_vec_tensor))])\n",
      "        self.M = theano.shared(np.eye(hidden_size).astype(theano.config.floatX))\n",
      "        \n",
      "        c_input = embeddings[c.flatten()].reshape((c.shape[0], c.shape[1], embeddings.shape[1]))\n",
      "        r_input = embeddings[r.flatten()].reshape((r.shape[0], r.shape[1], embeddings.shape[1]))                \n",
      "\n",
      "        l_in = lasagne.layers.InputLayer(shape=(batch_size, img_h, img_w))\n",
      "        \n",
      "        if is_bidirectional:\n",
      "            pass\n",
      "        else:\n",
      "            if use_lstm:\n",
      "                l_recurrent = lasagne.layers.LSTMLayer(l_in,\n",
      "                                                       hidden_size,\n",
      "                                                       backwards=False,\n",
      "                                                       learn_init=False,\n",
      "                                                       peepholes=True)\n",
      "            else:\n",
      "                if False:\n",
      "                    l_recurrent_in = lasagne.layers.InputLayer(shape=(batch_size, img_w))\n",
      "\n",
      "                    l_input_to_hidden = lasagne.layers.DenseLayer(l_recurrent_in,\n",
      "                                                                  hidden_size,\n",
      "                                                                  nonlinearity=None,\n",
      "                                                                  W=lasagne.init.Orthogonal())\n",
      "\n",
      "                    l_recurrent_hid = lasagne.layers.InputLayer(shape=(batch_size, hidden_size))\n",
      "\n",
      "                    l_hidden_to_hidden_1 = lasagne.layers.DenseLayer(l_recurrent_hid,\n",
      "                                                                     hidden_size,\n",
      "                                                                     nonlinearity=None,\n",
      "                                                                     W=lasagne.init.Orthogonal())\n",
      "\n",
      "                    \"\"\"\n",
      "                    l_hidden_to_hidden_2 = lasagne.layers.DenseLayer(l_hidden_to_hidden_1,\n",
      "                                                                     hidden_size, nonlinearity=None)\n",
      "                    \"\"\"\n",
      "\n",
      "                    l_recurrent = lasagne.layers.RecurrentLayer(l_in,\n",
      "                                                                l_input_to_hidden,\n",
      "                                                                l_hidden_to_hidden_1,\n",
      "                                                                nonlinearity=lasagne.nonlinearities.tanh,\n",
      "                                                                learn_init=True,\n",
      "                                                                backwards=False\n",
      "                                                                )\n",
      "                else:\n",
      "                    \"\"\"\n",
      "                    l_fwd = lasagne.layers.RecurrentLayer(l_in,\n",
      "                                                          hidden_size,\n",
      "                                                          nonlinearity=lasagne.nonlinearities.tanh,\n",
      "                                                          W_hid_to_hid=lasagne.init.Orthogonal(),\n",
      "                                                          W_in_to_hid=lasagne.init.Orthogonal(),\n",
      "                                                          backwards=False,\n",
      "                                                          learn_init=True\n",
      "                                                          )\n",
      "                    \n",
      "                    l_bck = lasagne.layers.RecurrentLayer(l_in,\n",
      "                                                          hidden_size,\n",
      "                                                          nonlinearity=lasagne.nonlinearities.tanh,\n",
      "                                                          W_hid_to_hid=lasagne.init.Orthogonal(),\n",
      "                                                          W_in_to_hid=lasagne.init.Orthogonal(),\n",
      "                                                          backwards=True,\n",
      "                                                          learn_init=True\n",
      "                                                          )\n",
      "                    \n",
      "                    l_recurrent = lasagne.layers.ElemwiseSumLayer([l_fwd, l_bck])  \n",
      "                    \"\"\"\n",
      "                    \n",
      "                    l_recurrent = lasagne.layers.RecurrentLayer(l_in,\n",
      "                                                          hidden_size,\n",
      "                                                          nonlinearity=lasagne.nonlinearities.tanh,\n",
      "                                                          W_hid_to_hid=lasagne.init.Orthogonal(),\n",
      "                                                          W_in_to_hid=lasagne.init.Orthogonal(),\n",
      "                                                          backwards=False,\n",
      "                                                          learn_init=True\n",
      "                                                          )                    \n",
      "        \n",
      "        if use_conv:\n",
      "            l_recurrent = lasagne.layers.ReshapeLayer(l_recurrent, (batch_size, 1, img_h, hidden_size))\n",
      "            #l_recurrent = lasagne.layers.DropoutLayer(l_recurrent, p=0.5)\n",
      "            conv_layers = []\n",
      "            for filter_size in filter_sizes:\n",
      "                conv_layer = lasagne.layers.Conv2DLayer(\n",
      "                        l_recurrent,\n",
      "                        num_filters=num_filters,\n",
      "                        filter_size=(filter_size, hidden_size),\n",
      "                        strides=(1,1),\n",
      "                        nonlinearity=lasagne.nonlinearities.rectify,\n",
      "                        border_mode='valid'\n",
      "                        )\n",
      "                pool_layer = lasagne.layers.MaxPool2DLayer(\n",
      "                        conv_layer,\n",
      "                        ds=(img_h-filter_size+1, 1)\n",
      "                        )\n",
      "                conv_layers.append(pool_layer)\n",
      "\n",
      "            #hidden_size = len(conv_layers) * num_filters\n",
      "            #hidden_size = num_filters\n",
      "            l_hidden1 = lasagne.layers.ConcatLayer(conv_layers)\n",
      "            l_hidden2 = lasagne.layers.DenseLayer(l_hidden1, num_units=hidden_size, nonlinearity=lasagne.nonlinearities.tanh)\n",
      "            l_out = l_hidden2\n",
      "        else:\n",
      "            l_out = l_recurrent\n",
      "        \n",
      "        if use_conv:\n",
      "            e_context = l_out.get_output(c_input, deterministic=False)\n",
      "            e_response = l_out.get_output(r_input, deterministic=False)\n",
      "        else:         \n",
      "            e_context = l_out.get_output(c_input, c_mask, deterministic=False)[T.arange(batch_size), c_seqlen].reshape((c.shape[0], hidden_size))   \n",
      "            e_response = l_out.get_output(r_input, r_mask, deterministic=False)[T.arange(batch_size), r_seqlen].reshape((r.shape[0], hidden_size))\n",
      "            \n",
      "        dp = T.batched_dot(e_context, T.dot(e_response, self.M.T))\n",
      "        #dp = pp('dp')(dp)\n",
      "        o = T.nnet.sigmoid(dp)\n",
      "        o = T.clip(o, 1e-7, 1.0-1e-7)\n",
      "\n",
      "        self.train_set_c = theano.shared(data['train']['c'], borrow=True)\n",
      "        self.train_set_r = theano.shared(data['train']['r'], borrow=True)\n",
      "        self.train_set_y = theano.shared(data['train']['y'], borrow=True)\n",
      "        self.train_set_c_mask = theano.shared(data['train']['c_mask'], borrow=True)\n",
      "        self.train_set_r_mask = theano.shared(data['train']['r_mask'], borrow=True)\n",
      "        self.train_set_c_seqlen = theano.shared(data['train']['c_seqlen'], borrow=True)\n",
      "        self.train_set_r_seqlen = theano.shared(data['train']['r_seqlen'], borrow=True)        \n",
      "        \n",
      "        self.val_set_c = theano.shared(data['val']['c'], borrow=True)\n",
      "        self.val_set_r = theano.shared(data['val']['r'], borrow=True)\n",
      "        self.val_set_y = theano.shared(data['val']['y'], borrow=True)\n",
      "        self.val_set_c_mask = theano.shared(data['val']['c_mask'], borrow=True)\n",
      "        self.val_set_r_mask = theano.shared(data['val']['r_mask'], borrow=True)\n",
      "        self.val_set_c_seqlen = theano.shared(data['val']['c_seqlen'], borrow=True)\n",
      "        self.val_set_r_seqlen = theano.shared(data['val']['r_seqlen'], borrow=True)\n",
      "        \n",
      "        self.test_set_c = theano.shared(data['test']['c'], borrow=True)\n",
      "        self.test_set_r = theano.shared(data['test']['r'], borrow=True)\n",
      "        self.test_set_y = theano.shared(data['test']['y'], borrow=True)\n",
      "        self.test_set_c_mask = theano.shared(data['test']['c_mask'], borrow=True)\n",
      "        self.test_set_r_mask = theano.shared(data['test']['r_mask'], borrow=True)\n",
      "        self.test_set_c_seqlen = theano.shared(data['test']['c_seqlen'], borrow=True)\n",
      "        self.test_set_r_seqlen = theano.shared(data['test']['r_seqlen'], borrow=True)          \n",
      "        \n",
      "        probas = T.concatenate([(1-o).reshape((-1,1)), o.reshape((-1,1))], axis=1)\n",
      "        pred = T.argmax(probas, axis=1)\n",
      "        errors = T.sum(T.neq(pred, y))    \n",
      "\n",
      "        cost = T.nnet.binary_crossentropy(o, y).mean()\n",
      "        params = lasagne.layers.get_all_params(l_out) + [self.M]\n",
      "        if non_static:\n",
      "            params += [embeddings]\n",
      "            \n",
      "        total_params = sum([p.get_value().size for p in params])\n",
      "        print \"total_params: \", total_params\n",
      "#        updates = lasagne.updates.adadelta(cost, params, learning_rate=1.0, rho=lr_decay)\n",
      "#        updates = sgd_updates_adadelta(cost, params, lr_decay, 1e-6, sqr_norm_lim)\n",
      "#        updates = lasagne.updates.nesterov_momentum(cost, params, learning_rate=0.1)\n",
      "        updates = adam(cost, params)\n",
      "                        \n",
      "        self.train_model = theano.function([index], cost, updates=updates,\n",
      "              givens={\n",
      "                c: self.train_set_c[index*batch_size:(index+1)*batch_size],\n",
      "                r: self.train_set_r[index*batch_size:(index+1)*batch_size],\n",
      "                y: self.train_set_y[index*batch_size:(index+1)*batch_size],\n",
      "                c_mask: self.train_set_c_mask[index*batch_size:(index+1)*batch_size],\n",
      "                r_mask: self.train_set_r_mask[index*batch_size:(index+1)*batch_size],\n",
      "                c_seqlen: self.train_set_c_seqlen[index*batch_size:(index+1)*batch_size],\n",
      "                r_seqlen: self.train_set_r_seqlen[index*batch_size:(index+1)*batch_size],\n",
      "              },\n",
      "              on_unused_input='warn')         \n",
      "\n",
      "        self.train_loss = theano.function([index], errors,\n",
      "                 givens={\n",
      "                    c: self.train_set_c[index * batch_size: (index + 1) * batch_size],\n",
      "                    r: self.train_set_r[index * batch_size: (index + 1) * batch_size],\n",
      "                    y: self.train_set_y[index * batch_size: (index + 1) * batch_size],\n",
      "                    c_mask: self.train_set_c_mask[index * batch_size: (index + 1) * batch_size],\n",
      "                    r_mask: self.train_set_r_mask[index * batch_size: (index + 1) * batch_size],\n",
      "                    c_seqlen: self.train_set_c_seqlen[index * batch_size: (index + 1) * batch_size],\n",
      "                    r_seqlen: self.train_set_r_seqlen[index * batch_size: (index + 1) * batch_size],\n",
      "                 },\n",
      "                 on_unused_input='warn')\n",
      "        \n",
      "        self.val_loss = theano.function([index], errors,\n",
      "             givens={\n",
      "                c: self.val_set_c[index * batch_size: (index + 1) * batch_size],\n",
      "                r: self.val_set_r[index * batch_size: (index + 1) * batch_size],\n",
      "                y: self.val_set_y[index * batch_size: (index + 1) * batch_size],\n",
      "                c_mask: self.val_set_c_mask[index * batch_size: (index + 1) * batch_size],\n",
      "                r_mask: self.val_set_r_mask[index * batch_size: (index + 1) * batch_size],\n",
      "                c_seqlen: self.val_set_c_seqlen[index * batch_size: (index + 1) * batch_size],\n",
      "                r_seqlen: self.val_set_r_seqlen[index * batch_size: (index + 1) * batch_size],\n",
      "             },\n",
      "             on_unused_input='warn')\n",
      "\n",
      "        self.test_loss = theano.function([index], errors,\n",
      "             givens={\n",
      "                c: self.test_set_c[index * batch_size: (index + 1) * batch_size],\n",
      "                r: self.test_set_r[index * batch_size: (index + 1) * batch_size],\n",
      "                y: self.test_set_y[index * batch_size: (index + 1) * batch_size],\n",
      "                c_mask: self.test_set_c_mask[index * batch_size: (index + 1) * batch_size],\n",
      "                r_mask: self.test_set_r_mask[index * batch_size: (index + 1) * batch_size],\n",
      "                c_seqlen: self.test_set_c_seqlen[index * batch_size: (index + 1) * batch_size],\n",
      "                r_seqlen: self.test_set_r_seqlen[index * batch_size: (index + 1) * batch_size],\n",
      "             },\n",
      "             on_unused_input='warn')\n",
      "\n",
      "    def train(self, n_epochs=100, shuffle_batch=True):\n",
      "        epoch = 0\n",
      "        best_val_perf = 0\n",
      "        val_perf = 0\n",
      "        test_perf = 0\n",
      "        cost_epoch = 0\n",
      "        \n",
      "        n_train_batches = self.data['train']['y'].shape[0] // self.batch_size\n",
      "        n_val_batches = self.data['val']['y'].shape[0] // self.batch_size\n",
      "        n_test_batches = self.data['test']['y'].shape[0] // self.batch_size\n",
      "\n",
      "        while (epoch < n_epochs):\n",
      "            epoch += 1\n",
      "            indices = range(n_train_batches)\n",
      "            if shuffle_batch:\n",
      "                indices = np.random.permutation(indices)\n",
      "            bar = pyprind.ProgBar(len(indices), monitor=True)\n",
      "            total_cost = 0\n",
      "            for minibatch_index in indices:\n",
      "                cost_epoch = self.train_model(minibatch_index)\n",
      "                total_cost += cost_epoch\n",
      "                self.set_zero(self.zero_vec)\n",
      "                bar.update()\n",
      "            print \"cost: \", (total_cost / len(indices))\n",
      "            train_losses = [self.train_loss(i) for i in xrange(n_train_batches)]\n",
      "            train_perf = 1 - np.sum(train_losses) / self.data['train']['y'].shape[0]\n",
      "            val_losses = [self.val_loss(i) for i in xrange(n_val_batches)]\n",
      "            val_perf = 1 - np.sum(val_losses) / self.data['val']['y'].shape[0]\n",
      "            print 'epoch %i, train_perf %f, val_perf %f' % (epoch, train_perf*100, val_perf*100)\n",
      "            if val_perf >= best_val_perf:\n",
      "                best_val_perf = val_perf\n",
      "                test_losses = [self.test_loss(i) for i in xrange(n_test_batches)]\n",
      "                test_perf = 1 - np.sum(test_losses) / self.data['test']['y'].shape[0]\n",
      "                print 'test_perf %f' % (test_perf*100)\n",
      "        return test_perf\n",
      "\n",
      "def as_floatX(variable):\n",
      "    if isinstance(variable, float):\n",
      "        return np.cast[theano.config.floatX](variable)\n",
      "\n",
      "    if isinstance(variable, np.ndarray):\n",
      "        return np.cast[theano.config.floatX](variable)\n",
      "    return theano.tensor.cast(variable, theano.config.floatX)\n",
      "\n",
      "def sgd_updates_adadelta(cost, params, rho=0.95, epsilon=1e-6, norm_lim=9, word_vec_name='embeddings'):\n",
      "    updates = OrderedDict({})\n",
      "    exp_sqr_grads = OrderedDict({})\n",
      "    exp_sqr_ups = OrderedDict({})\n",
      "    gparams = []\n",
      "    for param in params:\n",
      "        empty = np.zeros_like(param.get_value())\n",
      "        exp_sqr_grads[param] = theano.shared(value=as_floatX(empty),name=\"exp_grad_%s\" % param.name)\n",
      "        gp = T.grad(cost, param)\n",
      "        exp_sqr_ups[param] = theano.shared(value=as_floatX(empty), name=\"exp_grad_%s\" % param.name)\n",
      "        gparams.append(gp)\n",
      "    for param, gp in zip(params, gparams):\n",
      "        exp_sg = exp_sqr_grads[param]\n",
      "        exp_su = exp_sqr_ups[param]\n",
      "        up_exp_sg = rho * exp_sg + (1 - rho) * T.sqr(gp)\n",
      "        updates[exp_sg] = up_exp_sg\n",
      "        step =  -(T.sqrt(exp_su + epsilon) / T.sqrt(up_exp_sg + epsilon)) * gp\n",
      "        updates[exp_su] = rho * exp_su + (1 - rho) * T.sqr(step)\n",
      "        stepped_param = param + step\n",
      "        if (param.get_value(borrow=True).ndim == 2) and (param.name != word_vec_name):\n",
      "            col_norms = T.sqrt(T.sum(T.sqr(stepped_param), axis=0))\n",
      "            desired_norms = T.clip(col_norms, 0, T.sqrt(norm_lim))\n",
      "            scale = desired_norms / (1e-7 + col_norms)\n",
      "            updates[param] = stepped_param * scale\n",
      "        else:\n",
      "            updates[param] = stepped_param      \n",
      "    return updates \n",
      " \n",
      "def get_idx_from_sent(sent, word_idx_map, max_l, k):\n",
      "    \"\"\"\n",
      "    Transforms sentence into a list of indices. Pad with zeroes.\n",
      "    \"\"\"\n",
      "    x = []\n",
      "    words = sent.split()\n",
      "    for word in words[:max_l]:\n",
      "        if word in word_idx_map:\n",
      "            x.append(word_idx_map[word])\n",
      "    while len(x) < max_l:\n",
      "        x.append(0)\n",
      "    mask = np.zeros(max_l, dtype=np.bool)\n",
      "    mask[:len(words)] = 1\n",
      "    return x, mask, len(words) if len(words) < max_l-1 else max_l-1\n",
      "\n",
      "def make_idx_data(dataset, word_idx_map, max_l=152, k=300):\n",
      "    \"\"\"\n",
      "    Transforms sentences into a 2-d matrix.\n",
      "    \"\"\"\n",
      "    for i in xrange(len(dataset['y'])):\n",
      "        dataset['c'][i], dataset['c_mask'][i], dataset['c_seqlen'][i] = get_idx_from_sent(dataset['c'][i], word_idx_map, max_l, k)\n",
      "        dataset['r'][i], dataset['r_mask'][i], dataset['r_seqlen'][i] = get_idx_from_sent(dataset['r'][i], word_idx_map, max_l, k)\n",
      "    for col in ['c', 'r', 'y', 'c_seqlen', 'r_seqlen']:\n",
      "        dataset[col] = np.array(dataset[col], dtype=np.int32)\n",
      "    for col in ['c_mask', 'r_mask']:\n",
      "        dataset[col] = np.array(dataset[col], dtype=np.int8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pad_to_batch_size(X, batch_size):\n",
      "    n_seqs = X.shape[0]\n",
      "    seq_length = X.shape[1] if X.ndim > 1 else None\n",
      "    n_batches_out = np.ceil(float(n_seqs) / batch_size)\n",
      "    n_seqs_out = batch_size * n_batches_out\n",
      "\n",
      "    if X.ndim > 1:\n",
      "        X_out = np.zeros((n_seqs_out, seq_length), dtype=X.dtype)\n",
      "    else:\n",
      "        X_out = np.zeros((n_seqs_out), dtype=X.dtype)        \n",
      "    X_out[:n_seqs, ] = X\n",
      "    to_pad = n_seqs % batch_size\n",
      "    if to_pad > 0:\n",
      "        X_out[n_seqs:] = X[:batch_size-to_pad]\n",
      "    \n",
      "    return X_out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"loading data...\",\n",
      "data = cPickle.load(open('data.pkl', 'rb'))\n",
      "train_data, val_data, test_data, W, W2, word_idx_map, vocab = data\n",
      "print \"data loaded!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading data... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "data loaded!\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for key in ['c_mask', 'r_mask', 'c_seqlen', 'r_seqlen']:\n",
      "    for dataset in [train_data, val_data, test_data]:\n",
      "        dataset[key] = [0] * len(dataset['y'])\n",
      "\n",
      "make_idx_data(train_data, word_idx_map)\n",
      "make_idx_data(val_data, word_idx_map)\n",
      "make_idx_data(test_data, word_idx_map)\n",
      "\n",
      "BATCH_SIZE = 256\n",
      "\n",
      "for key in ['c', 'r', 'y', 'c_mask', 'r_mask', 'c_seqlen', 'r_seqlen']:\n",
      "    print key\n",
      "    for dataset in [train_data, val_data, test_data]:\n",
      "        dataset[key] = pad_to_batch_size(dataset[key], BATCH_SIZE)\n",
      "        print dataset[key].shape\n",
      "\n",
      "data = { 'train' : train_data, 'val': val_data, 'test': test_data }    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "c\n",
        "(100096, 152)\n",
        "(27904, 152)\n",
        "(17920, 152)\n",
        "r\n",
        "(100096, 152)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(27904, 152)\n",
        "(17920, 152)\n",
        "y\n",
        "(100096,)\n",
        "(27904,)\n",
        "(17920,)\n",
        "c_mask\n",
        "(100096, 152)\n",
        "(27904, 152)\n",
        "(17920, 152)\n",
        "r_mask\n",
        "(100096, 152)\n",
        "(27904, 152)\n",
        "(17920, 152)\n",
        "c_seqlen\n",
        "(100096,)\n",
        "(27904,)\n",
        "(17920,)\n",
        "r_seqlen\n",
        "(100096,)\n",
        "(27904,)\n",
        "(17920,)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rnn = RNN(data,\n",
      "          W.astype(theano.config.floatX),\n",
      "          img_w=300,\n",
      "          hidden_size=300,\n",
      "          batch_size=BATCH_SIZE,\n",
      "          lr_decay=0.95,\n",
      "          sqr_norm_lim=1,\n",
      "          non_static=True,\n",
      "          use_lstm=True,\n",
      "          use_conv=True)\n",
      "print rnn.train(n_epochs=100, shuffle_batch=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:198: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 3 is not part of the computational graph needed to compute the outputs: <TensorType(int32, vector)>.\n",
        "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
        "-c:198: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 4 is not part of the computational graph needed to compute the outputs: <TensorType(int8, matrix)>.\n",
        "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
        "-c:210: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 3 is not part of the computational graph needed to compute the outputs: <TensorType(int32, vector)>.\n",
        "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:210: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 4 is not part of the computational graph needed to compute the outputs: <TensorType(int8, matrix)>.\n",
        "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
        "-c:222: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 3 is not part of the computational graph needed to compute the outputs: <TensorType(int32, vector)>.\n",
        "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:222: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 4 is not part of the computational graph needed to compute the outputs: <TensorType(int8, matrix)>.\n",
        "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
        "-c:234: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 3 is not part of the computational graph needed to compute the outputs: <TensorType(int32, vector)>.\n",
        "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:234: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 4 is not part of the computational graph needed to compute the outputs: <TensorType(int8, matrix)>.\n",
        "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
        "0%                          100%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[                              ]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total_params:  43415100\n"
       ]
      },
      {
       "ename": "RuntimeError",
       "evalue": "CudaNdarray_ZEROS: allocation failed.\nApply node that caused the error: forall_inplace,gpu,grad_of_scan_fn&grad_of_scan_fn&grad_of_scan_fn}(Shape_i{0}.0, GpuElemwise{tanh,no_inplace}.0, GpuElemwise{mul,no_inplace}.0, GpuElemwise{Composite{(i0 - sqr(i1))},no_inplace}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuAlloc{memset_0=True}.0, GpuSubtensor{::int64}.0, DeepCopyOp.0, GpuSubtensor{::int64}.0, DeepCopyOp.0, GpuSubtensor{::int64}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuSubtensor{:int64:}.0, GpuAlloc{memset_0=True}.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, GpuJoin.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{1,0}.0, MakeVector.0)\nInputs types: [TensorType(int64, scalar), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, row), CudaNdarrayType(float32, row), CudaNdarrayType(float32, row), CudaNdarrayType(float32, matrix), TensorType(int64, vector)]\nInputs shapes: [(), (152, 256, 300), (152, 256, 300), (152, 256, 300), (152, 256, 1200), (152, 256, 300), (152, 256, 300), (152, 256, 300), (153, 256, 300), (153, 256, 300), (153, 256, 300), (153, 256, 300), (153, 256, 300), (153, 256, 300), (1, 300, 300), (1, 300, 300), (1, 300, 300), (1, 300, 300), (1, 300), (1, 300), (1, 300), (1, 300, 300), (1, 300, 300), (1, 300, 300), (1, 300, 300), (1, 300), (1, 300), (1, 300), (1, 300, 300), (1, 300, 300), (1, 300, 300), (1, 300, 300), (1, 300), (1, 300), (1, 300), (), (), (), (300, 1200), (1, 300), (1, 300), (1, 300), (1200, 300), (4,)]\nInputs strides: [(), (76800, 300, 1), (76800, 300, 1), (76800, 300, 1), (-307200, 1200, 1), (-76800, 300, 1), (-76800, 300, 1), (-76800, 300, 1), (76800, 300, 1), (-76800, 300, 1), (76800, 300, 1), (-76800, 300, 1), (76800, 300, 1), (-76800, 300, 1), (0, 300, 1), (0, 300, 1), (0, 300, 1), (0, 300, 1), (0, 1), (0, 1), (0, 1), (0, 300, 1), (0, 300, 1), (0, 300, 1), (0, 300, 1), (0, 1), (0, 1), (0, 1), (0, 300, 1), (0, 300, 1), (0, 300, 1), (0, 300, 1), (0, 1), (0, 1), (0, 1), (), (), (), (1200, 1), (0, 1), (0, 1), (0, 1), (1, 1200), (8,)]\nInputs values: [array(152), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', array(152), array(152), array(152), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', array([300, 300, 300, 300])]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-47-1bea1ecdcd21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m           \u001b[0muse_lstm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m           use_conv=True)\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-43-d02e9d606111>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_epochs, shuffle_batch)\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[0mtotal_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m                 \u001b[0mcost_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m                 \u001b[0mtotal_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_zero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/npow/code/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    607\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/npow/code/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/npow/code/Theano/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    646\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    647\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 648\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/npow/code/Theano/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    635\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    638\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/npow/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.9-64/scan_perform/scan_perform.so\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/npow/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.9-64/scan_perform/mod.cpp:4206)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;31mRuntimeError\u001b[0m: CudaNdarray_ZEROS: allocation failed.\nApply node that caused the error: forall_inplace,gpu,grad_of_scan_fn&grad_of_scan_fn&grad_of_scan_fn}(Shape_i{0}.0, GpuElemwise{tanh,no_inplace}.0, GpuElemwise{mul,no_inplace}.0, GpuElemwise{Composite{(i0 - sqr(i1))},no_inplace}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuAlloc{memset_0=True}.0, GpuSubtensor{::int64}.0, DeepCopyOp.0, GpuSubtensor{::int64}.0, DeepCopyOp.0, GpuSubtensor{::int64}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuSubtensor{:int64:}.0, GpuAlloc{memset_0=True}.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, DeepCopyOp.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, GpuJoin.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{1,0}.0, MakeVector.0)\nInputs types: [TensorType(int64, scalar), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, row), CudaNdarrayType(float32, row), CudaNdarrayType(float32, row), CudaNdarrayType(float32, matrix), TensorType(int64, vector)]\nInputs shapes: [(), (152, 256, 300), (152, 256, 300), (152, 256, 300), (152, 256, 1200), (152, 256, 300), (152, 256, 300), (152, 256, 300), (153, 256, 300), (153, 256, 300), (153, 256, 300), (153, 256, 300), (153, 256, 300), (153, 256, 300), (1, 300, 300), (1, 300, 300), (1, 300, 300), (1, 300, 300), (1, 300), (1, 300), (1, 300), (1, 300, 300), (1, 300, 300), (1, 300, 300), (1, 300, 300), (1, 300), (1, 300), (1, 300), (1, 300, 300), (1, 300, 300), (1, 300, 300), (1, 300, 300), (1, 300), (1, 300), (1, 300), (), (), (), (300, 1200), (1, 300), (1, 300), (1, 300), (1200, 300), (4,)]\nInputs strides: [(), (76800, 300, 1), (76800, 300, 1), (76800, 300, 1), (-307200, 1200, 1), (-76800, 300, 1), (-76800, 300, 1), (-76800, 300, 1), (76800, 300, 1), (-76800, 300, 1), (76800, 300, 1), (-76800, 300, 1), (76800, 300, 1), (-76800, 300, 1), (0, 300, 1), (0, 300, 1), (0, 300, 1), (0, 300, 1), (0, 1), (0, 1), (0, 1), (0, 300, 1), (0, 300, 1), (0, 300, 1), (0, 300, 1), (0, 1), (0, 1), (0, 1), (0, 300, 1), (0, 300, 1), (0, 300, 1), (0, 300, 1), (0, 1), (0, 1), (0, 1), (), (), (), (1200, 1), (0, 1), (0, 1), (0, 1), (1, 1200), (8,)]\nInputs values: [array(152), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', array(152), array(152), array(152), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', array([300, 300, 300, 300])]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}