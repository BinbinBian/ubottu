{
 "metadata": {
  "name": "",
  "signature": "sha256:dba8773cb13c2ea52ccb0ab46fc3b173bdc944959b3b4d3f7cae7736a3f4af62"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import csv\n",
      "import gzip\n",
      "import numpy as np\n",
      "import os\n",
      "import pandas as pd\n",
      "import re\n",
      "import sys\n",
      "from collections import defaultdict\n",
      "\n",
      "TRAIN_FILE = '../data/trainset_small.csv'\n",
      "VAL_FILE = '../data/valset.csv'\n",
      "TEST_FILE = '../data/testset.csv'\n",
      "\n",
      "W2V_FILE = '../embeddings/word2vec/GoogleNews-vectors-negative300.bin'\n",
      "GLOVE_FILE = '../embeddings/glove/glove.840B.300d.txt'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.environ['CLASSPATH']='.:../libs/commons-lang3-3.4.jar:../libs'\n",
      "from jnius import autoclass\n",
      "Twokenizer = autoclass('cmu.arktweetnlp.Twokenize')\n",
      "def tokenize(s, tokenizer=Twokenizer()):\n",
      "    tokens = tokenizer.tokenizeRawTweetText(s.decode('utf-8'))\n",
      "    return [tokens.get(i) for i in xrange(tokens.size())]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_file(fname, vocab, clean_string=True):\n",
      "    res = { 'c': [], 'r': [], 'y': [], 'wc_c': [], 'wc_r': []}\n",
      "    for line in csv.DictReader(open(fname)):\n",
      "        context, response, label = line['Context'], line['Response'], line['Correct']\n",
      "        if clean_string:\n",
      "            context = clean_str(context)\n",
      "            response = clean_str(response)\n",
      "        tok_context = tokenize(context)\n",
      "        tok_response = tokenize(response)\n",
      "        context = ' '.join(tok_context)\n",
      "        response = ' '.join(tok_response)\n",
      "        words_c = set(tok_context)\n",
      "        words_r = set(tok_response)\n",
      "        words = words_c | words_r\n",
      "        for word in words:\n",
      "            vocab[word] += 1\n",
      "        res['c'].append(context)\n",
      "        res['r'].append(response)\n",
      "        res['y'].append(label)\n",
      "        res['wc_c'].append(len(words_c))\n",
      "        res['wc_r'].append(len(words_c))\n",
      "    return res\n",
      "    \n",
      "def get_W(word_vecs, k):\n",
      "    \"\"\"\n",
      "    Get word matrix. W[i] is the vector for word indexed by i\n",
      "    \"\"\"\n",
      "    vocab_size = len(word_vecs)\n",
      "    word_idx_map = dict()\n",
      "    W = np.zeros(shape=(vocab_size+1, k))            \n",
      "    W[0] = np.zeros(k)\n",
      "    i = 1\n",
      "    for word in word_vecs:\n",
      "        W[i] = word_vecs[word]\n",
      "        word_idx_map[word] = i\n",
      "        i += 1\n",
      "    return W, word_idx_map\n",
      "\n",
      "def load_bin_vec(fname, vocab):\n",
      "    \"\"\"\n",
      "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
      "    \"\"\"\n",
      "    word_vecs = {}\n",
      "    with open(fname, \"rb\") as f:\n",
      "        header = f.readline()\n",
      "        vocab_size, layer1_size = map(int, header.split())\n",
      "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
      "        for line in xrange(vocab_size):\n",
      "            word = []\n",
      "            while True:\n",
      "                ch = f.read(1)\n",
      "                if ch == ' ':\n",
      "                    word = ''.join(word)\n",
      "                    break\n",
      "                if ch != '\\n':\n",
      "                    word.append(ch)   \n",
      "            if word in vocab:\n",
      "               word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
      "            else:\n",
      "                f.read(binary_len)\n",
      "    return word_vecs\n",
      "\n",
      "def load_glove_vec(fname, vocab):\n",
      "    \"\"\"\n",
      "    Loads word vecs from gloVe\n",
      "    \"\"\"\n",
      "    word_vecs = {}\n",
      "    with open(fname, \"rb\") as f:\n",
      "        for i,line in enumerate(f):\n",
      "            L = line.split()\n",
      "            word = L[0]\n",
      "            if word in vocab:\n",
      "                word_vecs[word] = np.array(L[1:], dtype=np.float32)\n",
      "    return word_vecs\n",
      "\n",
      "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
      "    \"\"\"\n",
      "    For words that occur in at least min_df documents, create a separate word vector.    \n",
      "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
      "    \"\"\"\n",
      "    f = open('unknown_words.txt', 'wb')\n",
      "    for word in vocab:\n",
      "        if word not in word_vecs and vocab[word] >= min_df:\n",
      "            word_vecs[word] = np.random.uniform(-0.25,0.25,k)  \n",
      "            f.write('%s\\n' % word)\n",
      "\n",
      "def clean_str(string, TREC=False):\n",
      "    \"\"\"\n",
      "    Tokenization/string cleaning for all datasets except for SST.\n",
      "    Every dataset is lower cased except for TREC\n",
      "    \"\"\"\n",
      "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
      "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
      "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
      "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
      "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
      "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
      "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
      "    string = re.sub(r\",\", \" , \", string) \n",
      "    string = re.sub(r\"!\", \" ! \", string) \n",
      "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
      "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
      "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
      "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
      "    return string.strip() if TREC else string.strip().lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"loading data...\"\n",
      "vocab = defaultdict(float)\n",
      "train_data = process_file(TRAIN_FILE, vocab)\n",
      "val_data = process_file(VAL_FILE, vocab)\n",
      "test_data = process_file(TEST_FILE, vocab)\n",
      "\n",
      "max_l = np.max(train_data['wc_c'] +\n",
      "               train_data['wc_r'] +\n",
      "               val_data['wc_c'] +\n",
      "               val_data['wc_r'] +\n",
      "               test_data['wc_c'] +\n",
      "               test_data['wc_r'])\n",
      "\n",
      "print \"data loaded!\"\n",
      "print \"num train: \", len(train_data)\n",
      "print \"num val: \", len(val_data)\n",
      "print \"num test: \", len(test_data)\n",
      "print \"vocab size: \", len(vocab)\n",
      "print \"max sentence length:\\n\", max_l\n",
      "\n",
      "print \"loading embeddings...\"\n",
      "#embeddings = load_bin_vec(W2V_FILE, vocab)\n",
      "embeddings = load_glove_vec(GLOVE_FILE, vocab)\n",
      "\n",
      "print \"embeddings loaded!\"\n",
      "print \"num words with embeddings: \", len(embeddings)\n",
      "\n",
      "rand_vecs = {}\n",
      "add_unknown_words(rand_vecs, vocab)\n",
      "W2, _ = get_W(rand_vecs, k=300)\n",
      "\n",
      "add_unknown_words(embeddings, vocab)\n",
      "W, word_idx_map = get_W(embeddings, k=300)\n",
      "\n",
      "for key in ['wc_c', 'wc_r']:\n",
      "    del train_data[key]\n",
      "    del val_data[key]\n",
      "    del test_data[key]\n",
      "\n",
      "cPickle.dump([train_data, val_data, test_data, W, W2, word_idx_map, vocab], open(\"data.pkl\", \"wb\"))\n",
      "print \"dataset created!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading data...\n",
        "data loaded!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "num train:  5\n",
        "num val:  5\n",
        "num test:  5\n",
        "vocab size:  34886\n",
        "max sentence length:\n",
        "165\n",
        "loading embeddings...\n",
        "embeddings loaded!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "num words with embeddings:  24484\n",
        "dataset created!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}