{
 "metadata": {
  "name": "",
  "signature": "sha256:139cc795aab576ebc58b1b94b07153e1bf6c31d6d23d7d3906367aaf65159d5b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import csv\n",
      "import gzip\n",
      "import numpy as np\n",
      "import os\n",
      "import pandas as pd\n",
      "import re\n",
      "import sys\n",
      "from collections import defaultdict\n",
      "\n",
      "TRAIN_FILE = '../data/trainset_small.csv'\n",
      "VAL_FILE = '../data/valset.csv'\n",
      "TEST_FILE = '../data/testset.csv'\n",
      "\n",
      "W2V_FILE = '../embeddings/word2vec/GoogleNews-vectors-negative300.bin'\n",
      "GLOVE_FILE = '../embeddings/glove/glove.840B.300d.txt'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.environ['CLASSPATH']='.:../libs/commons-lang3-3.4.jar:../libs'\n",
      "from jnius import autoclass\n",
      "Twokenizer = autoclass('cmu.arktweetnlp.Twokenize')\n",
      "def tokenize(s, tokenizer=Twokenizer()):\n",
      "    s = s.replace('</s>', '')\n",
      "    tokens = tokenizer.tokenizeRawTweetText(s.decode('utf-8'))\n",
      "    return [tokens.get(i) for i in xrange(tokens.size())]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_file(fname, vocab, clean_string=True):\n",
      "    res = { 'c': [], 'r': [], 'y': [], 'wc_c': [], 'wc_r': []}\n",
      "    for i,line in enumerate(csv.DictReader(open(fname))):\n",
      "        if i % 10000 == 0:\n",
      "            print i\n",
      "        context, response, label = line['Context'], line['Response'], line['Correct']\n",
      "        if clean_string:\n",
      "            context = clean_str(context)\n",
      "            response = clean_str(response)\n",
      "        tok_context = tokenize(context)\n",
      "        tok_response = tokenize(response)\n",
      "        context = ' '.join(tok_context)\n",
      "        response = ' '.join(tok_response)\n",
      "        words_c = set(tok_context)\n",
      "        words_r = set(tok_response)\n",
      "        words = words_c | words_r\n",
      "        for word in words:\n",
      "            vocab[word] += 1\n",
      "        res['c'].append(context)\n",
      "        res['r'].append(response)\n",
      "        res['y'].append(label)\n",
      "        res['wc_c'].append(len(words_c))\n",
      "        res['wc_r'].append(len(words_c))\n",
      "    return res\n",
      "    \n",
      "def get_W(word_vecs, k):\n",
      "    \"\"\"\n",
      "    Get word matrix. W[i] is the vector for word indexed by i\n",
      "    \"\"\"\n",
      "    vocab_size = len(word_vecs)\n",
      "    word_idx_map = dict()\n",
      "    W = np.zeros(shape=(vocab_size+1, k))            \n",
      "    W[0] = np.zeros(k)\n",
      "    i = 1\n",
      "    for word in word_vecs:\n",
      "        W[i] = word_vecs[word]\n",
      "        word_idx_map[word] = i\n",
      "        i += 1\n",
      "    return W, word_idx_map\n",
      "\n",
      "def load_bin_vec(fname, vocab):\n",
      "    \"\"\"\n",
      "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
      "    \"\"\"\n",
      "    word_vecs = {}\n",
      "    with open(fname, \"rb\") as f:\n",
      "        header = f.readline()\n",
      "        vocab_size, layer1_size = map(int, header.split())\n",
      "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
      "        for line in xrange(vocab_size):\n",
      "            word = []\n",
      "            while True:\n",
      "                ch = f.read(1)\n",
      "                if ch == ' ':\n",
      "                    word = ''.join(word)\n",
      "                    break\n",
      "                if ch != '\\n':\n",
      "                    word.append(ch)   \n",
      "            if word in vocab:\n",
      "               word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
      "            else:\n",
      "                f.read(binary_len)\n",
      "    return word_vecs\n",
      "\n",
      "def load_glove_vec(fname, vocab):\n",
      "    \"\"\"\n",
      "    Loads word vecs from gloVe\n",
      "    \"\"\"\n",
      "    word_vecs = {}\n",
      "    with open(fname, \"rb\") as f:\n",
      "        for i,line in enumerate(f):\n",
      "            L = line.split()\n",
      "            word = L[0]\n",
      "            if word in vocab:\n",
      "                word_vecs[word] = np.array(L[1:], dtype=np.float32)\n",
      "    return word_vecs\n",
      "\n",
      "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
      "    \"\"\"\n",
      "    For words that occur in at least min_df documents, create a separate word vector.    \n",
      "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
      "    \"\"\"\n",
      "    f = open('unknown_words.txt', 'wb')\n",
      "    for word in vocab:\n",
      "        if word not in word_vecs and vocab[word] >= min_df:\n",
      "            word_vecs[word] = np.random.uniform(-0.25,0.25,k)  \n",
      "            f.write('%s\\n' % word)\n",
      "\n",
      "def clean_str(string, TREC=False):\n",
      "    \"\"\"\n",
      "    Tokenization/string cleaning for all datasets except for SST.\n",
      "    Every dataset is lower cased except for TREC\n",
      "    \"\"\"\n",
      "    #string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
      "    string = re.sub(r\"\\'m\", \" \\'m\", string) \n",
      "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
      "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
      "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
      "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
      "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
      "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
      "    string = re.sub(r\"`\", \" ` \", string)\n",
      "    string = re.sub(r\",\", \" , \", string) \n",
      "    #string = re.sub(r\"!\", \" ! \", string) \n",
      "    #string = re.sub(r\"\\(\", \" \\( \", string) \n",
      "    #string = re.sub(r\"\\)\", \" \\) \", string) \n",
      "    #string = re.sub(r\"\\?\", \" \\? \", string) \n",
      "    #string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
      "    return string.strip() if TREC else string.strip().lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"loading data...\"\n",
      "vocab = defaultdict(float)\n",
      "train_data = process_file(TRAIN_FILE, vocab)\n",
      "val_data = process_file(VAL_FILE, vocab)\n",
      "test_data = process_file(TEST_FILE, vocab)\n",
      "\n",
      "max_l = np.max(train_data['wc_c'] +\n",
      "               train_data['wc_r'] +\n",
      "               val_data['wc_c'] +\n",
      "               val_data['wc_r'] +\n",
      "               test_data['wc_c'] +\n",
      "               test_data['wc_r'])\n",
      "\n",
      "print \"data loaded!\"\n",
      "print \"num train: \", len(train_data)\n",
      "print \"num val: \", len(val_data)\n",
      "print \"num test: \", len(test_data)\n",
      "print \"vocab size: \", len(vocab)\n",
      "print \"max sentence length:\\n\", max_l\n",
      "\n",
      "print \"loading embeddings...\"\n",
      "#embeddings = load_bin_vec(W2V_FILE, vocab)\n",
      "embeddings = load_glove_vec(GLOVE_FILE, vocab)\n",
      "\n",
      "print \"embeddings loaded!\"\n",
      "print \"num words with embeddings: \", len(embeddings)\n",
      "\n",
      "rand_vecs = {}\n",
      "add_unknown_words(rand_vecs, vocab)\n",
      "W2, _ = get_W(rand_vecs, k=300)\n",
      "\n",
      "add_unknown_words(embeddings, vocab)\n",
      "W, word_idx_map = get_W(embeddings, k=300)\n",
      "\n",
      "for key in ['wc_c', 'wc_r']:\n",
      "    del train_data[key]\n",
      "    del val_data[key]\n",
      "    del test_data[key]\n",
      "\n",
      "cPickle.dump([train_data, val_data, test_data, W, W2, word_idx_map, vocab], open(\"data.pkl\", \"wb\"))\n",
      "print \"dataset created!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading data...\n",
        "0\n",
        "10000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "70000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "80000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "90000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data loaded!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "num train:  5\n",
        "num val:  5\n",
        "num test:  5\n",
        "vocab size:  140507\n",
        "max sentence length:\n",
        "345\n",
        "loading embeddings...\n",
        "embeddings loaded!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "num words with embeddings:  67724\n",
        "dataset created!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "L=\"\"\"\n",
      "https://bbs.archlinux.org/viewtopic.php?pid=1000281#p1000281   shows that you should check your lxde-rc.xml?\n",
      "W: GPG-fout: http://nl.archive.ubuntu.com oneiric-backports Release: De volgende handtekeningen waren ongeldig: BADSIG 40976EAF437D05B5 Ubuntu Archive Automatic Signing Key <ftpmaster@ubuntu.com>\n",
      "there are 340,282,366,920,938,463,463,374,607,431,768,211,456 unique passphrases. the data is gone http://blog.dustinkirkland.com/2009/02/how-encrypted-home-ecryptfs-works.html\n",
      "http://askubuntu.com/questions/29328/how-do-i-increase-the-text-size-of-the-text-on-a-console     also seems good.\n",
      "Last update: February 26, 2010 https://code.google.com/p/supertux/source/browse/README     yeah... that IS the git... click the checkout link...\n",
      "sudo apt-get install svn; svn co http://path.to.svn/path/to/src\n",
      "http://linuxmce.iptp.org/snapshots/\n",
      "https://gist.github.com/1ffdcbf2300f5c08f27a\n",
      "http://paste.ubuntu.com/880787/ < can't install grub\n",
      "http://paste.ubuntu.com/880784/ << fdisk -l\n",
      "gotta admit I've never even heard of an .ogm file.  But I did find this:  http://mygeekopinions.blogspot.com/2011/06/install-lives-in-ubuntu-1104-natty.html\n",
      "IP=`wget -q -O - http://ip.keithscode.com`; echo $IP      will give your routers WAN IP. .If you ask for the wrong thing youo will get the wrong reply\n",
      "https://help.ubuntu.com/8.04/serverguide/C/mysql.html => MySQL | https://help.ubuntu.com/11.10/serverguide/C/phpmyadmin.html => PHPMyAdmin\n",
      "The /etc/X11/xorg.conf file is deprecated, but sometimes may still be needed to pass values to specific drivers. Generic xorg.conf generation: http://ubottu.com/y/xorgconf - ATI/AMD ( fglrx driver ) specific: http://ubottu.com/y/atiamd - NVidia ( nvidia driver )specific: http://ubottu.com/y/nvidia man xorg.conf for file structure and syntax.\n",
      "did you try this, bootdelay ??? >>> http://ubuntuforums.org/showpost.php?p=10307755&postcount=29\n",
      "https://www.google.com/search?client=ubuntu&channel=fs&q=printer+reset+chips&ie=utf-8&oe=utf-8\n",
      "http://www.pasteall.org/pic/40511 I booted and i'm in gparted, just not sure what to do now?\n",
      "Linux is the kernel (core) of the Ubuntu operating system. Many operating systems use Linux as a kernel. For more information on Linux in general, visit http://en.wikipedia.org/wiki/Linux\n",
      "dget -us -uc --build http://archive.ubuntu.com/ubuntu/pool/main/i/imagemagick/imagemagick_6.7.7.10-2ubuntu4.dsc\n",
      "http://paste.ubuntu.com/1397671/ aborted :(\n",
      "when running LANG=C;sudo apt-get --purge autoremove -f -1.11 I got this outputhttp://paste.ubuntu.com/1046314/\n",
      "the channel is #httpd Kevin\n",
      " installed shorewall with Ubuntu Server 12.04 and in the rules, I allowed incoming HTTP and SSH connections\n",
      "Since it's VMWare, you should probably adjust the resolution of the guest system, as per http://goo.gl/oOHnS\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def is_number(s):\n",
      "    try:\n",
      "        float(s)\n",
      "        return True\n",
      "    except ValueError:\n",
      "        return False\n",
      "\n",
      "def replace_sentence(text):\n",
      "    words = nltk.word_tokenize(text)\n",
      "    sent = nltk.pos_tag(words)\n",
      "    chunks = nltk.ne_chunk(sent, binary=False)\n",
      "    sentence = []\n",
      "    nodelist = ['PERSON','ORGANIZATION','GPE','LOCATION','FACILITY','GSP']\n",
      "    for c,word in zip(chunks,words):\n",
      "        changed = False\n",
      "        if hasattr(c, 'label'):     \n",
      "            if c.label() in nodelist:\n",
      "                print c.label()\n",
      "                sentence.append(\"__%s__\" % c.label())\n",
      "                changed = True\n",
      "        if not changed:\n",
      "            if word.startswith('http://') or word.startswith('https://'):\n",
      "                sentence.append(\"__URL__\")\n",
      "            elif word.isdigit():\n",
      "                sentence.append(\"__NUM__\")\n",
      "            elif os.path.isabs(word):\n",
      "                sentence.append(\"__PATH__\")\n",
      "            else:\n",
      "                print \"HI\"\n",
      "                sentence.append(word)\n",
      "            \n",
      "    return \" \".join(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "for line in L.split('\\n'):\n",
      "    line = clean_str(line)\n",
      "    print \"line: \", line\n",
      "    print \"clean: \", process_line(line)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "line:  \n",
        "clean:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[]\n",
        "line:  https://bbs.archlinux.org/viewtopic.php?pid=1000281#p1000281   shows that you should check your lxde-rc.xml?\n",
        "clean:  ('https://bbs.archlinux.org/viewtopic.php?pid=1000281#p1000281', '-NONE-') https://bbs.archlinux.org/viewtopic.php?pid=1000281#p1000281\n",
        "('shows', 'VBZ') shows\n",
        "('that', 'IN') that\n",
        "('you', 'PRP') you\n",
        "('should', 'MD') should\n",
        "('check', 'VB') check\n",
        "('your', 'PRP$') your\n",
        "('lxde-rc', 'JJ') lxde-rc\n",
        "('.', '.') .\n",
        "('xml', 'NN') xml\n",
        "('?', '.') ?\n",
        "['**URL**', 'shows', 'that', 'you', 'should', 'check', 'your', 'lxde-rc', '.', 'xml', '?']\n",
        "line:  w: gpg-fout: http://nl.archive.ubuntu.com oneiric-backports release: de volgende handtekeningen waren ongeldig: badsig 40976eaf437d05b5 ubuntu archive automatic signing key <ftpmaster@ubuntu.com>\n",
        "clean:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('w', 'NN') w\n",
        "(':', ':') :\n",
        "('gpg-fout', 'IN') gpg-fout\n",
        "(':', ':') :\n",
        "('http://nl.archive.ubuntu.com', 'JJ') http://nl.archive.ubuntu.com\n",
        "('oneiric-backports', 'NNS') oneiric-backports\n",
        "('release', 'NN') release\n",
        "(':', ':') :\n",
        "('de', 'IN') de\n",
        "('volgende', 'NN') volgende\n",
        "('handtekeningen', 'NN') handtekeningen\n",
        "('waren', 'NNS') waren\n",
        "('ongeldig', 'VBP') ongeldig\n",
        "(':', ':') :\n",
        "('badsig', 'JJ') badsig\n",
        "('40976eaf437d05b5', 'CD') 40976eaf437d05b5\n",
        "('ubuntu', 'NN') ubuntu\n",
        "('archive', 'JJ') archive\n",
        "('automatic', 'JJ') automatic\n",
        "('signing', 'NN') signing\n",
        "('key', 'JJ') key\n",
        "('<', 'NN') <\n",
        "('ftpmaster@ubuntu.com', 'NN') ftpmaster@ubuntu.com\n",
        "('>', ':') >\n",
        "['w', ':', 'gpg-fout', ':', '**URL**', 'oneiric-backports', 'release', ':', 'de', 'volgende', 'handtekeningen', 'waren', 'ongeldig', ':', 'badsig', '40976eaf437d05b5', 'ubuntu', 'archive', 'automatic', 'signing', 'key', '<', 'ftpmaster@ubuntu.com', '>']\n",
        "line:  there are 340 , 282 , 366 , 920 , 938 , 463 , 463 , 374 , 607 , 431 , 768 , 211 , 456 unique passphrases. the data is gone http://blog.dustinkirkland.com/2009/02/how-encrypted-home-ecryptfs-works.html\n",
        "clean:  ('there', 'EX') there\n",
        "('are', 'VBP') are\n",
        "('340', 'CD') 340\n",
        "(',', ',') ,\n",
        "('282', 'CD') 282\n",
        "(',', ',') ,\n",
        "('366', 'CD') 366\n",
        "(',', ',') ,\n",
        "('920', 'CD') 920\n",
        "(',', ',') ,\n",
        "('938', 'CD') 938\n",
        "(',', ',') ,\n",
        "('463', 'CD') 463\n",
        "(',', ',') ,\n",
        "('463', 'CD') 463\n",
        "(',', ',') ,\n",
        "('374', 'CD') 374\n",
        "(',', ',') ,\n",
        "('607', 'CD') 607\n",
        "(',', ',') ,\n",
        "('431', 'CD') 431\n",
        "(',', ',') ,\n",
        "('768', 'CD') 768\n",
        "(',', ',') ,\n",
        "('211', 'CD') 211\n",
        "(',', ',') ,\n",
        "('456', 'CD') 456\n",
        "('unique', 'NN') unique\n",
        "('passphrases', 'NNS') passphrases\n",
        "('.', '.') .\n",
        "('the', 'DT') the\n",
        "('data', 'NNS') data\n",
        "('is', 'VBZ') is\n",
        "('gone', 'VBN') gone\n",
        "('http://blog.dustinkirkland.com/2009/02/how-encrypted-home-ecryptfs-works.html', 'JJ') http://blog.dustinkirkland.com/2009/02/how-encrypted-home-ecryptfs-works.html\n",
        "['there', 'are', '**NUMBER**', ',', '**NUMBER**', ',', '**NUMBER**', ',', '**NUMBER**', ',', '**NUMBER**', ',', '**NUMBER**', ',', '**NUMBER**', ',', '**NUMBER**', ',', '**NUMBER**', ',', '**NUMBER**', ',', '**NUMBER**', ',', '**NUMBER**', ',', '**NUMBER**', 'unique', 'passphrases', '.', 'the', 'data', 'is', 'gone', '**URL**']\n",
        "line:  http://askubuntu.com/questions/29328/how-do-i-increase-the-text-size-of-the-text-on-a-console     also seems good.\n",
        "clean:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('http://askubuntu.com/questions/29328/how-do-i-increase-the-text-size-of-the-text-on-a-console', 'JJ') http://askubuntu.com/questions/29328/how-do-i-increase-the-text-size-of-the-text-on-a-console\n",
        "('also', 'RB') also\n",
        "('seems', 'VBZ') seems\n",
        "('good', 'JJ') good\n",
        "('.', '.') .\n",
        "['**URL**', 'also', 'seems', 'good', '.']\n",
        "line:  last update: february 26 ,  2010 https://code.google.com/p/supertux/source/browse/readme     yeah... that is the git... click the checkout link...\n",
        "clean:  ('last', 'JJ') last\n",
        "('update', 'NN') update\n",
        "(':', ':') :\n",
        "('february', 'JJ') february\n",
        "('26', 'CD') 26\n",
        "(',', ',') ,\n",
        "('2010', 'CD') 2010\n",
        "('https://code.google.com/p/supertux/source/browse/readme', 'JJ') https://code.google.com/p/supertux/source/browse/readme\n",
        "('yeah', 'NN') yeah\n",
        "('...', ':') ...\n",
        "('that', 'IN') that\n",
        "('is', 'VBZ') is\n",
        "('the', 'DT') the\n",
        "('git', 'NN') git\n",
        "('...', ':') ...\n",
        "('click', 'NN') click\n",
        "('the', 'DT') the\n",
        "('checkout', 'NN') checkout\n",
        "('link', 'NN') link\n",
        "('...', ':') ...\n",
        "['last', 'update', ':', 'february', '**NUMBER**', ',', '**NUMBER**', '**URL**', 'yeah', '...', 'that', 'is', 'the', 'git', '...', 'click', 'the', 'checkout', 'link', '...']\n",
        "line:  sudo apt-get install svn; svn co http://path.to.svn/path/to/src\n",
        "clean:  ('sudo', 'NN') sudo\n",
        "('apt-get', 'NN') apt-get\n",
        "('install', 'NN') install\n",
        "('svn', 'NN') svn\n",
        "(';', ':') ;\n",
        "('svn', 'NN') svn\n",
        "('co', 'NN') co\n",
        "('http://path.to.svn/path/to/src', 'NN') http://path.to.svn/path/to/src\n",
        "['sudo', 'apt-get', 'install', 'svn', ';', 'svn', 'co', '**URL**']\n",
        "line:  http://linuxmce.iptp.org/snapshots/\n",
        "clean:  ('http://linuxmce.iptp.org/snapshots/', 'NN') http://linuxmce.iptp.org/snapshots/\n",
        "['**URL**']\n",
        "line:  https://gist.github.com/1ffdcbf2300f5c08f27a\n",
        "clean:  ('https://gist.github.com/1ffdcbf2300f5c08f27a', 'NN') https://gist.github.com/1ffdcbf2300f5c08f27a\n",
        "['**URL**']\n",
        "line:  http://paste.ubuntu.com/880787/ < ca n't install grub\n",
        "clean:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('http://paste.ubuntu.com/880787/', 'NN') http://paste.ubuntu.com/880787/\n",
        "('<', '``') <\n",
        "('ca', 'MD') ca\n",
        "(\"n't\", 'RB') n't\n",
        "('install', 'VB') install\n",
        "('grub', 'NN') grub\n",
        "['**URL**', '<', 'ca', \"n't\", 'install', 'grub']\n",
        "line:  http://paste.ubuntu.com/880784/ << fdisk -l\n",
        "clean:  ('http://paste.ubuntu.com/880784/', 'NN') http://paste.ubuntu.com/880784/\n",
        "('<<', '``') <<\n",
        "('fdisk', 'NN') fdisk\n",
        "('-l', 'NN') -l\n",
        "['**URL**', '<<', 'fdisk', '-l']\n",
        "line:  gotta admit i 've never even heard of an .ogm file.  but i did find this:  http://mygeekopinions.blogspot.com/2011/06/install-lives-in-ubuntu-1104-natty.html\n",
        "clean:  ('gotta', 'NN') gotta\n",
        "('admit', 'VBD') admit\n",
        "('i', 'PRP') i\n",
        "(\"'\", \"''\") '\n",
        "('ve', 'VBP') ve\n",
        "('never', 'RB') never\n",
        "('even', 'RB') even\n",
        "('heard', 'VBN') heard\n",
        "('of', 'IN') of\n",
        "('an', 'DT') an\n",
        "('.', '.') .\n",
        "('ogm', 'NN') ogm\n",
        "('file', 'NN') file\n",
        "('.', '.') .\n",
        "('but', 'CC') but\n",
        "('i', 'PRP') i\n",
        "('did', 'VBD') did\n",
        "('find', 'VB') find\n",
        "('this', 'DT') this\n",
        "(':', ':') :\n",
        "('http://mygeekopinions.blogspot.com/2011/06/install-lives-in-ubuntu-1104-natty.html', 'JJ') http://mygeekopinions.blogspot.com/2011/06/install-lives-in-ubuntu-1104-natty.html\n",
        "['gotta', 'admit', 'i', \"'\", 've', 'never', 'even', 'heard', 'of', 'an', '.', 'ogm', 'file', '.', 'but', 'i', 'did', 'find', 'this', ':', '**URL**']\n",
        "line:  ip= ` wget -q -o - http://ip.keithscode.com ` ; echo $ip      will give your routers wan ip. .if you ask for the wrong thing youo will get the wrong reply\n",
        "clean:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('ip', 'NN') ip\n",
        "('=', '``') =\n",
        "('`', '``') `\n",
        "('wget', 'NN') wget\n",
        "('-q', 'NN') -q\n",
        "('-o', '-NONE-') -o\n",
        "('-', ':') -\n",
        "('http://ip.keithscode.com', 'JJ') http://ip.keithscode.com\n",
        "('`', '``') `\n",
        "(';', ':') ;\n",
        "('echo', 'NN') echo\n",
        "('$ip', 'NN') $ip\n",
        "('will', 'MD') will\n",
        "('give', 'VB') give\n",
        "('your', 'PRP$') your\n",
        "('routers', 'NNS') routers\n",
        "('wan', 'VBP') wan\n",
        "('ip', 'NN') ip\n",
        "('.', '.') .\n",
        "('.', '.') .\n",
        "('if', 'IN') if\n",
        "('you', 'PRP') you\n",
        "('ask', 'VBP') ask\n",
        "('for', 'IN') for\n",
        "('the', 'DT') the\n",
        "('wrong', 'JJ') wrong\n",
        "('thing', 'NN') thing\n",
        "('youo', 'NN') youo\n",
        "('will', 'MD') will\n",
        "('get', 'VB') get\n",
        "('the', 'DT') the\n",
        "('wrong', 'JJ') wrong\n",
        "('reply', 'NN') reply\n",
        "['ip', '=', '`', 'wget', '-q', '-o', '-', '**URL**', '`', ';', 'echo', '$ip', 'will', 'give', 'your', 'routers', 'wan', 'ip', '.', '.', 'if', 'you', 'ask', 'for', 'the', 'wrong', 'thing', 'youo', 'will', 'get', 'the', 'wrong', 'reply']\n",
        "line:  https://help.ubuntu.com/8.04/serverguide/c/mysql.html => mysql | https://help.ubuntu.com/11.10/serverguide/c/phpmyadmin.html => phpmyadmin\n",
        "clean:  ('https://help.ubuntu.com/8.04/serverguide/c/mysql.html', 'JJ') https://help.ubuntu.com/8.04/serverguide/c/mysql.html\n",
        "('=>', 'NN') =>\n",
        "('mysql', 'NN') mysql\n",
        "('|', ':') |\n",
        "('https://help.ubuntu.com/11.10/serverguide/c/phpmyadmin.html', 'JJ') https://help.ubuntu.com/11.10/serverguide/c/phpmyadmin.html\n",
        "('=>', 'NN') =>\n",
        "('phpmyadmin', 'NN') phpmyadmin\n",
        "['**URL**', '=>', 'mysql', '|', '**URL**', '=>', 'phpmyadmin']\n",
        "line:  the /etc/x11/xorg.conf file is deprecated ,  but sometimes may still be needed to pass values to specific drivers. generic xorg.conf generation: http://ubottu.com/y/xorgconf - ati/amd ( fglrx driver ) specific: http://ubottu.com/y/atiamd - nvidia ( nvidia driver )specific: http://ubottu.com/y/nvidia man xorg.conf for file structure and syntax.\n",
        "clean:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('the', 'DT') the\n",
        "('/etc/x11/xorg', 'JJ') /etc/x11/xorg\n",
        "('.', '.') .\n",
        "('conf', 'NN') conf\n",
        "('file', 'NN') file\n",
        "('is', 'VBZ') is\n",
        "('deprecated', 'VBN') deprecated\n",
        "(',', ',') ,\n",
        "('but', 'CC') but\n",
        "('sometimes', 'RB') sometimes\n",
        "('may', 'MD') may\n",
        "('still', 'RB') still\n",
        "('be', 'VB') be\n",
        "('needed', 'VBN') needed\n",
        "('to', 'TO') to\n",
        "('pass', 'VB') pass\n",
        "('values', 'NNS') values\n",
        "('to', 'TO') to\n",
        "('specific', 'JJ') specific\n",
        "('drivers', 'NNS') drivers\n",
        "('.', '.') .\n",
        "('generic', 'JJ') generic\n",
        "('xorg', 'NN') xorg\n",
        "('.', '.') .\n",
        "('conf', 'NN') conf\n",
        "('generation', 'NN') generation\n",
        "(':', ':') :\n",
        "('http://ubottu.com/y/xorgconf', 'JJ') http://ubottu.com/y/xorgconf\n",
        "('-', ':') -\n",
        "(GPE ati/amd/JJ) ati/amd\n",
        "('(', 'NN') (\n",
        "('fglrx', 'NN') fglrx\n",
        "('driver', 'NN') driver\n",
        "(')', ':') )\n",
        "('specific', 'JJ') specific\n",
        "(':', ':') :\n",
        "('http://ubottu.com/y/atiamd', 'JJ') http://ubottu.com/y/atiamd\n",
        "('-', ':') -\n",
        "('nvidia', 'NN') nvidia\n",
        "('(', ':') (\n",
        "('nvidia', 'NN') nvidia\n",
        "('driver', 'NN') driver\n",
        "(')', ':') )\n",
        "('specific', 'JJ') specific\n",
        "(':', ':') :\n",
        "('http://ubottu.com/y/nvidia', 'JJ') http://ubottu.com/y/nvidia\n",
        "('man', 'NN') man\n",
        "('xorg', 'NN') xorg\n",
        "('.', '.') .\n",
        "('conf', 'NN') conf\n",
        "('for', 'IN') for\n",
        "('file', 'NN') file\n",
        "('structure', 'NN') structure\n",
        "('and', 'CC') and\n",
        "('syntax', 'NN') syntax\n",
        "('.', '.') .\n",
        "['the', '**PATH**', '.', 'conf', 'file', 'is', 'deprecated', ',', 'but', 'sometimes', 'may', 'still', 'be', 'needed', 'to', 'pass', 'values', 'to', 'specific', 'drivers', '.', 'generic', 'xorg', '.', 'conf', 'generation', ':', '**URL**', '-', 'ati/amd', '(', 'fglrx', 'driver', ')', 'specific', ':', '**URL**', '-', 'nvidia', '(', 'nvidia', 'driver', ')', 'specific', ':', '**URL**', 'man', 'xorg', '.', 'conf', 'for', 'file', 'structure', 'and', 'syntax', '.']\n",
        "line:  did you try this ,  bootdelay ??? >>> http://ubuntuforums.org/showpost.php?p=10307755&postcount=29\n",
        "clean:  ('did', 'VBD') did\n",
        "('you', 'PRP') you\n",
        "('try', 'VBP') try\n",
        "('this', 'DT') this\n",
        "(',', ',') ,\n",
        "('bootdelay', 'VBP') bootdelay\n",
        "('???', '.') ???\n",
        "('>>>', ':') >>>\n",
        "('http://ubuntuforums.org/showpost.php?p=10307755&postcount=29', '-NONE-') http://ubuntuforums.org/showpost.php?p=10307755&postcount=29\n",
        "['did', 'you', 'try', 'this', ',', 'bootdelay', '???', '>>>', '**URL**']\n",
        "line:  https://www.google.com/search?client=ubuntu&channel=fs&q=printer+reset+chips&ie=utf-8&oe=utf-8\n",
        "clean:  ('https://www.google.com/search?client=ubuntu&channel=fs&q=printer+reset+chips&ie=utf-8&oe=utf-8', 'NN') https://www.google.com/search?client=ubuntu&channel=fs&q=printer+reset+chips&ie=utf-8&oe=utf-8\n",
        "['**URL**']\n",
        "line:  http://www.pasteall.org/pic/40511 i booted and i 'm in gparted ,  just not sure what to do now?\n",
        "clean:  ('http://www.pasteall.org/pic/40511', 'NN') http://www.pasteall.org/pic/40511\n",
        "('i', 'PRP') i\n",
        "('booted', 'VBD') booted\n",
        "('and', 'CC') and\n",
        "('i', 'PRP') i\n",
        "(\"'\", \"''\") '\n",
        "('m', 'NN') m\n",
        "('in', 'IN') in\n",
        "('gparted', 'JJ') gparted\n",
        "(',', ',') ,\n",
        "('just', 'RB') just\n",
        "('not', 'RB') not\n",
        "('sure', 'JJ') sure\n",
        "('what', 'WP') what\n",
        "('to', 'TO') to\n",
        "('do', 'VB') do\n",
        "('now', 'RB') now\n",
        "('?', '.') ?\n",
        "['**URL**', 'i', 'booted', 'and', 'i', \"'\", 'm', 'in', 'gparted', ',', 'just', 'not', 'sure', 'what', 'to', 'do', 'now', '?']\n",
        "line:  linux is the kernel (core) of the ubuntu operating system. many operating systems use linux as a kernel. for more information on linux in general ,  visit http://en.wikipedia.org/wiki/linux\n",
        "clean:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('linux', 'NN') linux\n",
        "('is', 'VBZ') is\n",
        "('the', 'DT') the\n",
        "('kernel', 'NN') kernel\n",
        "('(', ':') (\n",
        "('core', 'NN') core\n",
        "(')', ':') )\n",
        "('of', 'IN') of\n",
        "('the', 'DT') the\n",
        "('ubuntu', 'NN') ubuntu\n",
        "('operating', 'NN') operating\n",
        "('system', 'NN') system\n",
        "('.', '.') .\n",
        "('many', 'JJ') many\n",
        "('operating', 'NN') operating\n",
        "('systems', 'NNS') systems\n",
        "('use', 'VBP') use\n",
        "('linux', 'NN') linux\n",
        "('as', 'IN') as\n",
        "('a', 'DT') a\n",
        "('kernel', 'NN') kernel\n",
        "('.', '.') .\n",
        "('for', 'IN') for\n",
        "('more', 'JJR') more\n",
        "('information', 'NN') information\n",
        "('on', 'IN') on\n",
        "('linux', 'NN') linux\n",
        "('in', 'IN') in\n",
        "('general', 'JJ') general\n",
        "(',', ',') ,\n",
        "('visit', 'NN') visit\n",
        "('http://en.wikipedia.org/wiki/linux', '-NONE-') http://en.wikipedia.org/wiki/linux\n",
        "['linux', 'is', 'the', 'kernel', '(', 'core', ')', 'of', 'the', 'ubuntu', 'operating', 'system', '.', 'many', 'operating', 'systems', 'use', 'linux', 'as', 'a', 'kernel', '.', 'for', 'more', 'information', 'on', 'linux', 'in', 'general', ',', 'visit', '**URL**']\n",
        "line:  dget -us -uc --build http://archive.ubuntu.com/ubuntu/pool/main/i/imagemagick/imagemagick_6.7.7.10-2ubuntu4.dsc\n",
        "clean:  ('dget', 'NN') dget\n",
        "('-us', 'JJ') -us\n",
        "('-uc', 'NN') -uc\n",
        "('--', ':') --\n",
        "('build', 'NN') build\n",
        "('http://archive.ubuntu.com/ubuntu/pool/main/i/imagemagick/imagemagick_6.7.7.10-2ubuntu4.dsc', 'JJ') http://archive.ubuntu.com/ubuntu/pool/main/i/imagemagick/imagemagick_6.7.7.10-2ubuntu4.dsc\n",
        "['dget', '-us', '-uc', '--', 'build', '**URL**']\n",
        "line:  http://paste.ubuntu.com/1397671/ aborted :(\n",
        "clean:  ('http://paste.ubuntu.com/1397671/', 'NN') http://paste.ubuntu.com/1397671/\n",
        "('aborted', 'VBD') aborted\n",
        "(':(', 'CD') :(\n",
        "['**URL**', 'aborted', ':(']\n",
        "line:  when running lang=c;sudo apt-get --purge autoremove -f -1.11 i got this outputhttp://paste.ubuntu.com/1046314/\n",
        "clean:  ('when', 'WRB') when\n",
        "('running', 'VBG') running\n",
        "('lang', 'NN') lang\n",
        "('=', ':') =\n",
        "('c', 'NN') c\n",
        "(';', ':') ;\n",
        "('sudo', 'NN') sudo\n",
        "('apt-get', 'NN') apt-get\n",
        "('--', ':') --\n",
        "('purge', 'NN') purge\n",
        "('autoremove', 'VBP') autoremove\n",
        "('-f', 'JJ') -f\n",
        "('-', ':') -\n",
        "('1.11', 'CD') 1.11\n",
        "('i', 'PRP') i\n",
        "('got', 'VBD') got\n",
        "('this', 'DT') this\n",
        "('output', 'NN') output\n",
        "('http://paste.ubuntu.com/1046314/', 'NN') http://paste.ubuntu.com/1046314/\n",
        "['when', 'running', 'lang', '=', 'c', ';', 'sudo', 'apt-get', '--', 'purge', 'autoremove', '-f', '-', '**NUMBER**', 'i', 'got', 'this', 'output', '**URL**']\n",
        "line:  the channel is #httpd kevin\n",
        "clean:  ('the', 'DT') the\n",
        "('channel', 'NN') channel\n",
        "('is', 'VBZ') is\n",
        "('#httpd', 'JJ') #httpd\n",
        "('kevin', 'NN') kevin\n",
        "['the', 'channel', 'is', '#httpd', 'kevin']\n",
        "line:  installed shorewall with ubuntu server 12.04 and in the rules ,  i allowed incoming http and ssh connections\n",
        "clean:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('installed', 'VBD') installed\n",
        "('shorewall', 'VBN') shorewall\n",
        "('with', 'IN') with\n",
        "('ubuntu', 'NN') ubuntu\n",
        "('server', 'NN') server\n",
        "('12.04', 'CD') 12.04\n",
        "('and', 'CC') and\n",
        "('in', 'IN') in\n",
        "('the', 'DT') the\n",
        "('rules', 'NNS') rules\n",
        "(',', ',') ,\n",
        "('i', 'PRP') i\n",
        "('allowed', 'VBD') allowed\n",
        "('incoming', 'VBG') incoming\n",
        "('http', 'NN') http\n",
        "('and', 'CC') and\n",
        "('ssh', 'JJ') ssh\n",
        "('connections', 'NNS') connections\n",
        "['installed', 'shorewall', 'with', 'ubuntu', 'server', '**NUMBER**', 'and', 'in', 'the', 'rules', ',', 'i', 'allowed', 'incoming', 'http', 'and', 'ssh', 'connections']\n",
        "line:  since it 's vmware ,  you should probably adjust the resolution of the guest system ,  as per http://goo.gl/oohns\n",
        "clean:  ('since', 'IN') since\n",
        "('it', 'PRP') it\n",
        "(\"'\", 'POS') '\n",
        "('s', 'NNS') s\n",
        "('vmware', 'VBP') vmware\n",
        "(',', ',') ,\n",
        "('you', 'PRP') you\n",
        "('should', 'MD') should\n",
        "('probably', 'RB') probably\n",
        "('adjust', 'VB') adjust\n",
        "('the', 'DT') the\n",
        "('resolution', 'NN') resolution\n",
        "('of', 'IN') of\n",
        "('the', 'DT') the\n",
        "('guest', 'JJS') guest\n",
        "('system', 'NN') system\n",
        "(',', ',') ,\n",
        "('as', 'IN') as\n",
        "('per', 'NN') per\n",
        "('http://goo.gl/oohns', 'NNS') http://goo.gl/oohns\n",
        "['since', 'it', \"'\", 's', 'vmware', ',', 'you', 'should', 'probably', 'adjust', 'the', 'resolution', 'of', 'the', 'guest', 'system', ',', 'as', 'per', '**URL**']\n",
        "line:  \n",
        "clean:  []\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = replace_sentence('Abraham Lincoln kevin')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PERSON\n",
        "ORGANIZATION\n",
        "HI\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "'**PERSON** **ORGANIZATION** kevin'"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenize(\"___EOS___ kevin\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "['___EOS___', 'kevin']"
       ]
      }
     ],
     "prompt_number": 56
    }
   ],
   "metadata": {}
  }
 ]
}